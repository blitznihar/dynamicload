{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget for environment selection\n",
    "interface_dropdown = widgets.Dropdown(\n",
    "    options=[\"LAWSON-0005\", \"LAWSON-0006\", \"LAWSON-0007\", \"LAWSON-0008\"],\n",
    "    value=\"LAWSON-0006\",\n",
    "    description=\"Interface:\",\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "# display(widgets.HBox([interface_dropdown, outputdirectory]))\n",
    "display(widgets.HBox([interface_dropdown]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# Database connection parameters\n",
    "db_user = 'root'\n",
    "db_password = 'strong_password'\n",
    "db_host = 'localhost'\n",
    "db_name = 'yourdb'\n",
    "db_port = 33062\n",
    "interface_id = interface_dropdown.value\n",
    "\n",
    "# Create a database connection\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "print(interface_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to read data for a specific interfaceID\n",
    "queryInterface = f\"SELECT * FROM interface WHERE interface_id = '{interface_id}'\"\n",
    "# Load data into dataframe\n",
    "dfInterface = pd.read_sql(queryInterface, engine)\n",
    "display(dfInterface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to read data for a specific interfaceID\n",
    "queryInterface = f\"SELECT * FROM interface WHERE interface_id = '{interface_id}'\"\n",
    "\n",
    "print(queryInterface)\n",
    "\n",
    "\n",
    "\n",
    "# Load data into dataframe\n",
    "dfInterface = pd.read_sql(queryInterface, engine)\n",
    "\n",
    "mappingId = dfInterface['mapping_id'].astype(str).to_string(index=False)\n",
    "\n",
    "# Query to read data for a specific interfaceID\n",
    "queryMapping = f\"SELECT * FROM mapping WHERE mapping_id = '{mappingId}'\"\n",
    "\n",
    "# Load data into dataframe\n",
    "dfMapping = pd.read_sql(queryMapping, engine)\n",
    "\n",
    "display(dfInterface)\n",
    "display(dfMapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Fixed Length File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delimiter = dfMapping['source_delimiter'].to_string(index=False)\n",
    "print(dfInterface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#LOGIC FOR FIXED WIDTH FILE\n",
    "if(dfInterface['source_type'].head(1).to_string(index=False).upper()== 'FIXED'):\n",
    "    colspecs = [(row['start_position'], row['start_position'] + row['source_length']) for index, row in dfMapping.iterrows() if row['source_field'] is not None]\n",
    "    names = [row['source_field'] for index, row in dfMapping.iterrows() if row['source_field'] is not None]  # Column names\n",
    "    dfSource = pd.read_fwf('SourceFileFixedWidth.dat', colspecs=colspecs, names=names, header=None)\n",
    "\n",
    "#LOGIC FOR DELIMITED FILE\n",
    "elif(dfInterface['source_type'].to_string(index=False).upper() == 'DELIMITED'):\n",
    "    delimiter = dfInterface['source_delimiter'].to_string(index=False)\n",
    "    print(delimiter)\n",
    "    names = [row['source_field'] for index, row in dfMapping.iterrows() if row['source_field'] is not None]  # Column names\n",
    "    sourcefile = dfInterface['source_path'].to_string(index=False)\n",
    "    dfSource = pd.read_csv('SampleFiles/05.TXT', delimiter=delimiter, names=names, header=None)\n",
    "    # column_types = {'col1': int, 'col2': str, 'col3': str, 'col3': float}\n",
    "# dfSource3 = pd.read_csv('Delimited.csv', delimiter=',', header=None, dtype=column_types)\n",
    "    \n",
    "#dfSource = pd.read_csv('SourceFileDelimited.dat', delimiter='|', names=names, header=None)\n",
    "\n",
    "print(dfSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the same columns as dfSource\n",
    "dfDestination = pd.DataFrame(columns=dfSource.columns)\n",
    "#Solidify the Mapping logic and data transformation\n",
    "    \n",
    "\n",
    "destinationFields = ''\n",
    "for index, row in dfMapping.iterrows():\n",
    "    destination = row['destination_field']\n",
    "    source = row['source_field']\n",
    "    mapping_function = row['mapping_function']\n",
    "    destination_type = row['destination_data_type']\n",
    "    if mapping_function == 'PASSTHROUGH':\n",
    "        if index == len(dfMapping) - 1:\n",
    "            destinationFields += destination\n",
    "        else:\n",
    "            destinationFields += destination + ','\n",
    "    if mapping_function is None:\n",
    "        if index == len(dfMapping) - 1:\n",
    "            destinationFields += f'{source} AS {destination}'\n",
    "        else:\n",
    "            destinationFields += f'{source} AS {destination},'\n",
    "    else:\n",
    "        if index == len(dfMapping) - 1:\n",
    "            destinationFields += f'{mapping_function} AS {destination}'\n",
    "        else:\n",
    "            destinationFields += f'{mapping_function} AS {destination},'\n",
    "\n",
    "destinationQuery = f\"SELECT {destinationFields} FROM dfSource\"\n",
    "print(destinationQuery)\n",
    "print(destinationFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as psql\n",
    "\n",
    "dfDestination = psql.sqldf(destinationQuery, globals())\n",
    "\n",
    "print(dfDestination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load into Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the destination table name from the interface table\n",
    "destination_table = dfInterface['destination_type'].iloc[0]\n",
    "print(destination_table)\n",
    "# Load the dataframe into the SQL table\n",
    "dfDestination.to_sql(destination_table, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(f\"Data loaded into table {destination_table} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
